{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_load_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWuVeGl7dcw1HMzvqBT50H"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ezuvtvx3HKR"
      },
      "source": [
        "# in this example we will look at \n",
        "# 1. the training task \n",
        "# 2. the evaluation task \n",
        "# 3. the training loop \n",
        "!pip install trax\n",
        "import trax\n",
        "from trax.supervised import training \n",
        "import trax.layers as tl\n",
        "import os \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKVODduo9T5a"
      },
      "source": [
        "# create the streams from the tensorflow datasets \n",
        "train_stream = trax.data.TFDS(\"imdb_reviews\", keys=(\"text\", \"label\"), train=True)()\n",
        "eval_stream = trax.data.TFDS(\"imdb_reviews\", keys=(\"text\", \"label\"), train=False)()\n",
        "\n",
        "# building up the pipeline \n",
        "data_pipeline=trax.data.Serial(\n",
        "    trax.data.Tokenize(vocab_file=\"en_8k.subword\", keys=[0]),\n",
        "    trax.data.Shuffle(), \n",
        "    trax.data.FilterByLength(max_length=2048, length_keys=[0]), \n",
        "    trax.data.Shuffle(),\n",
        "    trax.data.FilterByLength(max_length=2048, length_keys=[0]),\n",
        "    trax.data.BucketByLength(boundaries=[32,128,512,2048],\n",
        "                             batch_sizes=[512,128,32,8,1],\n",
        "                             length_keys=[0]),\n",
        "    trax.data.AddLossWeights()\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_batches_stream=data_pipeline(train_stream)\n",
        "\n",
        "eval_batches_stream=data_pipeline(eval_stream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3JDtM0gGGY8"
      },
      "source": [
        "import trax.data as data\n",
        "sentiment_analysis_model=tl.Serial(\n",
        "    tl.Embedding(data.vocab_size(vocab_file=\"en_8k.subword\"), d_feature=256),\n",
        "    tl.Mean(axis=1),\n",
        "    tl.Dense(2), #classifies 2 classes \n",
        "    tl.LogSoftmax()\n",
        ")\n",
        "print(sentiment_analysis_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f62nmOTF8Hqe"
      },
      "source": [
        "\n",
        "# create an output directory for weights and checkpoints \n",
        "# plug in our model training/evaluation \n",
        "# use a loop to iterate over each instances eval_batches_stream= data_pipeline(eval_stream)\n",
        "# training task \n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_batches_stream, \n",
        "    loss_layer=tl.CrossEntropyLoss(), \n",
        "    optimizer=trax.optimizers.Adam(0.01), \n",
        "    n_steps_per_checkpoint=200, \n",
        ")\n",
        "\n",
        "# evaluation \n",
        "eval_task=training.EvalTask(\n",
        "    labeled_data = eval_batches_stream,\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], \n",
        "    n_eval_batches=20\n",
        ")\n",
        "\n",
        "# create checkpoints \n",
        "output_dir = os.path.expanduser(\"~/output_dir/\")\n",
        "!rm -rf {output_dir}\n",
        "training_loop=training.Loop(sentiment_analysis_model,\n",
        "                            train_task,\n",
        "                            eval_tasks=[eval_task],\n",
        "                            output_dir=output_dir)\n",
        "# set how many loops to run \n",
        "training_loop.run(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSnaw5XX9SWF"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgKvSvEn8xUd"
      },
      "source": [
        "import numpy \n",
        "example_input= \"this was a decent film that I enjoyed watching. It passed some spare time nicely\"\n",
        "\n",
        "# tokenize \n",
        "input_iter = iter([example_input])\n",
        "input_tokens = data.tokenize(input_iter, vocab_file=\"en_8k.subword\")\n",
        "tokenized_input = list(input_tokens)[0]\n",
        "#placeholder branch input \n",
        "tokenized_with_batch = tokenized_input[None, :]\n",
        "# extract log probabilites \n",
        "sentiment_prob_logs = sentiment_analysis_model(tokenized_with_batch)\n",
        "# normalise the logs \n",
        "norm_log_prob=numpy.exp(sentiment_prob_logs)\n",
        "# extract sentiment polarity \n",
        "sentiment=numpy.argmax(norm_log_prob[0])\n",
        "print('Input review:\\n\"{}\"\\nThe sentiment is: {}'.format(example_input, \"Positive\" if sentiment else \"Negative\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SYUntnwP-s7",
        "outputId": "10cd25a0-fe41-401c-f6f1-15f409c1d23c"
      },
      "source": [
        "# loading from a checkpoint \n",
        "training_loop.load_checkpoint(directory=\"~/output_dir/\", filename='model.pkl.gz')\n",
        "# take up training from a given location (2000) and run another 200 times \n",
        "training_loop.run(200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/lib/xla_bridge.py:374: UserWarning: jax.host_id has been renamed to jax.process_index. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_id has been renamed to jax.process_index. This alias \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step   2200: Ran 200 train steps in 11.42 secs\n",
            "Step   2200: train CrossEntropyLoss |  0.28224170\n",
            "Step   2200: eval  CrossEntropyLoss |  0.38194130\n",
            "Step   2200: eval          Accuracy |  0.84218750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W1xZY9zQGFz",
        "outputId": "a189a42b-6f76-4b3d-e189-659d3c91c6eb"
      },
      "source": [
        "# loading a pretrained model \n",
        "new_model = tl.Serial(\n",
        "    tl.Embedding(data.vocab_size(vocab_file='en_8k.subword'), d_feature=256),\n",
        "    tl.Mean(axis=1), \n",
        "    tl.Dense(2),\n",
        "    tl.LogSoftmax()\n",
        ")\n",
        "\n",
        "import numpy\n",
        "def parse_sentiment(text, new_model):\n",
        "  input_iter = iter([text])\n",
        "  input_tokens=data.tokenize(input_iter, vocab_file=\"en_8k.subword\")\n",
        "  tokenize_input=list(input_tokens)[0]\n",
        "  tokenize_with_batch = tokenize_input[None, :]\n",
        "  sentiment_log_probs=new_model(tokenize_with_batch)\n",
        "  norm_log_probs=numpy.exp(sentiment_log_probs)\n",
        "  sentiment=numpy.argmax(norm_log_probs[0])\n",
        "  return sentiment\n",
        "# initialise this new model with weights from the old on e\n",
        "new_model.init_from_file(file_name=\"/root/output_dir/model.pkl.gz\", weights_only=True)\n",
        "print(\"the sentiemt is : \", parse_sentiment(\"this was a decent film that I enjoyed watching. It passed some spare time nicely\", new_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the sentiemt is :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPLahYbeRqn0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}